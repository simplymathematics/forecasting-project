---
title: "Data 624 Project 2"
author:


output:
  html_document:
    df_print: paged
---
# Final Project

### Problem

Data related to the manufacturing process of a soft drink are provided, including values
of various parameters that control the process. The objective is to build a predictive model to predict the
pH content based on the manufacturing process data.


### Executive Summary

<TODO: Briefly explain methodology, and summarize findings and conclusions. Which models worked best and what were the main predictors?>


### Dependencies

Our code requires the following dependencies.

```{r libs, eval=T, echo=T, warning=F, message=F}
library(caret)
library(caTools)
library(corrplot)
library(e1071)
library(fastDummies)
library(forecast)
library(ggplot2)
library(imputeTS)
library(lattice)
library(knitr)
library(ModelMetrics)
library(nnet)
library(randomForest)
library(readxl)
library(reshape2)
library(tidyr)
library(tidyverse)
library(xlsx)
```

### Data Initialization and Preprocessing

Below we can see a sample of the data, as read from Excel files. Our target variable is pH and each data vector, except for brand code, is numeric. However, we can convert this to a numeric vector easily. There are also some missing values that must be imputed. The only non-numeric data is the brand code. All lettered codes have been replaced with the equivalent integers
($A \rightarrow 1, B \rightarrow 2, ... $). Likewise, the NAs have been replaced with 0 such that they don't contribute to the regression function.


```{r init1, eval=T, echo=T, warning=F, message=F}
training <- read_excel("StudentData.xlsx")
test     <- read_excel("StudentEvaluation.xlsx")
 
# Wrongly assumed that I didn't need dummy vars
#old <- c("A", "B", "C", "D", "NA")
#new <- c( 1 ,  2 ,  3 ,  4 ,  0 )

#for (i in 1:length(old)){
#  training$`Brand Code` <- gsub(old[i], new[i], training$`Brand Code`)
#}

training <- dummy_cols(training, select_columns = training$Brand.Code)
test <- dummy_cols(test, select_columns = test$Brand.Code)


training <- as.data.frame(sapply(training, as.numeric))
test     <- as.data.frame(sapply(test, as.numeric))

names(training) <- make.names(names(training))
names(test) <- make.names(names(test))

training$Brand.Code <- NULL
test$Brand.Code <- NULL
head(training)
```


### Imputation of Missing Values

Below we impute the missing values using a monotone cubic approximator (known as a Stineman interpolation). It has a tendency to perform well on linear as well as higher-order data vectors.

```{r impute1, eval=T, echo=T, warning=F, message=F}
training <- na.interpolation(training, option = 'stine')
test <- na.interpolation(test, option = 'stine')
```

Below we can see the mean and standard deviation for each data vector. As we can see, our data occurrs across many orders of magnitude. For the best fit, it should be 

```{r mean_sd, eval=T, echo=T, warning=F, message=F}
means <- sapply(training, mean, na.rm = TRUE)
sds   <- sapply(training, sd, na.rm = TRUE)
explore <- as.data.frame(cbind( means, sds))
ggplot(explore, aes(x = row.names(explore), y = means))+ 
  geom_bar(stat = 'identity') + 
  labs(title = "Means of Various Features") + 
  xlab("Data Features") + 
  ylab("Mean of Data") +
  theme(panel.background = element_blank()) + 
  geom_errorbar(aes(ymin = means - sds, ymax = means + sds))
```


```{r figure1, fig.height=10}
ggplot(data = gather(training), mapping = aes(x = value)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white")+
  geom_density(alpha=.2, fill="lightgrey")+
  facet_wrap(~key, ncol = 1, scales = 'free') 
```

### Correlation Plot of Predictors

```{r corrplot, eval=T, echo=T, warning=F, message=F}
results <- cor(training, method = 'pearson')
corrplot::corrplot(results, method = 'circle')
```



### Checking for Missing Values

```{r missing_val, eval=T, echo=T, warning=F, message=F}
## Response variable missing value 
#table(is.na(training$PH))
## remove rows with missing response variable
training <- training[!is.na(training$PH), ]

## near zero value predictors
nzv <- nearZeroVar(training, saveMetrics= TRUE)
nzv[nzv$nzv,]

```


### Data Pre-processing

Remove near zero predictors, filling in missing values with knn method, transform predictors using YeoJohnson method, center and scale the data as well.

```{r pre_proc, eval=T, echo=T, warning=F, message=F}
x_train <- subset(training, select = -PH )
y_train <- training$PH

x_test <- subset(test, select = -PH )
y_test <- test$PH

preProcValues <- preProcess(x_train, method = c("center", "scale", "YeoJohnson", "nzv", "corr"))

trainTransformed <- predict(preProcValues, x_train)
testTransformed <- predict(preProcValues, x_test)
```



### Support Vector Machine Model

TODO: This section needs to be enabled. rmse values need to be calculated.

```{r svm1, eval=T, echo=T, warning=F, message=F}
exec = FALSE
if (exec) {
set.seed(100)
fitControl <- trainControl(## 10-fold CV
                           method = "repeatedcv",
                           number = 3,
                           ## repeated ten times
                           repeats = 3)
svmFit <- train(trainTransformed, y_train,
                 method = "svmRadial",
                 trControl = fitControl,
                 tuneLength = 8,
                 metric = "RMSE")
svmFit
model1 <- svmFit
}
```

### Stochastic Gradient Boosting 

```{r}
gbmGrid <-  expand.grid(interaction.depth = c(1, 5, 9), 
                        n.trees = (1:30)*50, 
                        shrinkage = 0.1,
                        n.minobsinnode = 20)
                        
nrow(gbmGrid)

set.seed(123)
gbmFit <- train(trainTransformed, y_train,
                 method = "gbm", 
                 trControl = fitControl, 
                 verbose = FALSE, 
                 tuneGrid = gbmGrid,
                 ## Specify which metric to optimize
                 metric = "RMSE")
gbmFit

```

### Random Forest Model

Random forests are a modification of bagging that builds a large collection of de-correlated
trees [1]. They are considered to belong in the category of non-parametric models since 
the number of parameters grows with the size of the training set. They are considered to be
an improvement to the use of CART (Classification and Regression Tree) models because they
do not suffer from some of the problems associated with CART models, such as the fact that
CART models are unstable: small changes to the structure of the input data can have large
effects on the CART model [2]. Random forests are designed to be low-variance estimators.

Random forests are based on the basic idea of aggregating uncorrelated sets of predictors,
since one way to reduce the variance of an estimate is to average several estimates together [2].
A random forest trains a randomly chosen set of input variables over a randomly chosen subset of
the data, and aggregates together several such trees to produce an overall estimator. Random
forests have proven to be quite successful in a variety of real-world applications and often
are seen to generalize very well to unseen real-world data.


```{r RF1, eval=T, echo=T, warning=F, message=F}

# Split the training data into a portion that is withheld from the model and used to evaluate
# the model.

set.seed(123)
sample = sample.split(training$PH, SplitRatio = .75)
training_forest = subset(training, sample == TRUE)
test_forest  = subset(training, sample == FALSE)

set.seed(123)
training_forest <- training_forest %>% dplyr::select(-Brand.Code)
rfFit <- randomForest::randomForest(PH ~ ., data = training_forest, importance = TRUE,
                                    ntree = 100, keep.forest = TRUE)

varImpPlot(rfFit, n.var=20,
           main="Important Variables in Random Forest Model (top 20 shown)")

model2 <- rfFit
```


We now compute RMSE values for the Random Forest model on both the training and the test (withheld) portion of the data set.

```{r RF_rmse, eval=T, echo=T, warning=F, message=F}
training_forest2 = dplyr::select(training_forest, -PH)
rfPred.train = predict(rfFit, training_forest2)
rmse.rf.train = rmse(training_forest$PH, rfPred.train)

test_forest2 = dplyr::select(test_forest, -PH, -Brand.Code)
rfPred.test = predict(rfFit, test_forest2)
rmse.rf.test = rmse(test_forest$PH, rfPred.test)

kable(data.frame(Model=c("Random Forest"), RMSE.train=c(rmse.rf.train), RMSE.test=c(rmse.rf.test)))
```


### Neural Network

We next build and evaluate a Neural Network model of the regression problem.

```{r nnet1, eval=T, echo=T, warning=F, message=F}
set.seed(123)
training_nn = training_forest2
training_nn_ph = training_forest$PH
test_nn = test_forest2
test_nn_ph = test_forest$PH
  
nnetFit <- nnet(training_nn, training_nn_ph,
                size = 4,
                decay = 0.01,
                linout = TRUE,
                trace = FALSE,
                maxit = 500, # Iterations
                ## Number of parameters used by the model
                MaxNWts= 4 * (ncol(training_nn) + 1) + 5 + 1)
nnetFit
model3 <- nnetFit
rmse.nnet.train = rmse(training_nn_ph, predict(nnetFit, training_nn))
rmse.nnet.test = rmse(test_nn_ph, predict(nnetFit, test_nn))
kable(data.frame(Model=c("ANN"), RMSE.train=c(rmse.nnet.train), RMSE.test=c(rmse.nnet.test)))
```
# Generalized Linear Models

```{r}

glm_label <- training$PH
glm_train <- select(training, -PH)

model4 <- glm(glm_label ~., glm_train, family = "gaussian")
model5 <- step(model4, direction = "backward")
model6 <- step(model5, direction = "backward")
model7 <- step(model6, direction = "backward")



logistic_label <- sigmoid(glm_label)

model8 <- glm(logistic_label ~., glm_train, family = "binomial")
model9 <- step(model8, direction = "backward")
model10 <- step(model9, direction = "backward")


sum4 <- summary(model4)
sum5 <- summary(model5)
sum6 <- summary(model6)
sum7 <- summary(model7)
sum8 <- summary(model8)
sum9 <- summary(model9)
sum10 <- summary(model10)
 
```



### Comparison of Models

We now compare and rank the RMSE errors produced by the various models on the portion of the training data that was withheld from the training of the models. Using the model with the smallest RMSE on the withheld training data, we also make a final prediction on the test data provided. The predictions are written to an Excel file.

Since we see that the Random Forest model produced the lowest RMSE on the withheld training data, we select it as the best model to predict PH in the manufacturing process data set. A final set of predictions is made using this model.

# TODO: Add SVM/GLMs to Analysis

```{r compare_and_predict, eval=T, echo=T, warning=F, message=F}
modelperf = data.frame(matrix(ncol=3, nrow=6))
colnames(modelperf) = c("Dataset", "Model", "RMSE")

rmse.svm.train = rmse.svm.test = 0

modelperf[1,] = list("Train", "Random Forest", rmse.rf.train)
modelperf[2,] = list("Test", "Random Forest", rmse.rf.test)
modelperf[3,] = list("Train", "ANN", rmse.nnet.train)
modelperf[4,] = list("Test", "ANN", rmse.nnet.test)
modelperf[5,] = list("Train", "SVM", rmse.svm.train)
modelperf[6,] = list("Test", "SVM", rmse.svm.test)

ggplot(data=modelperf, aes(x=Model, y=RMSE, fill=Dataset)) +
    geom_bar(stat="identity", position=position_dodge()) +
    ggtitle("Model Performance for PH Prediction")

bestFit = rfFit
PH.pred = predict(bestFit, test)
write.xlsx(PH.pred, file="DATA624.xlsx", sheetName="PH", append=F)
```


### Conclusion


### References

1. Random Forests. https://uc-r.github.io/random_forests
2. Kevin Murphy (2012). Machine Learning a Probabilistic Perspective.
3. Support Vector Machine. https://uc-r.github.io/svm
4. Support Vector Machines. http://web.mit.edu/6.034/wwwbob/svm.pdf
